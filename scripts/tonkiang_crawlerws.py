#!/usr/bin/env python3
"""
Tonkiang.us IPTVçˆ¬è™« - æŒ‰å«è§†é¢‘é“å·æœç´¢å¹¶è¾“å‡ºå¯¹åº”é“¾æ¥ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
"""

import requests
import re
import os
import time
import random
import hashlib
from datetime import datetime
import concurrent.futures
from threading import Lock

class TonkiangCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.base_url = "https://tonkiang.us/"
        self.request_timeout = (5, 15)
        self.all_links = []  # å­˜å‚¨æ‰€æœ‰æ‰¾åˆ°çš„é“¾æ¥
        self.lock = Lock()  # çº¿ç¨‹å®‰å…¨é”
        
        # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æå‡è§£ææ•ˆç‡
        self.m3u8_pattern = re.compile(r'https?://[^\s<>"]+?\.m3u8(?:\?[^\s<>"]*)?', re.IGNORECASE)
        self.onclick_pattern = re.compile(r'onclick="glshle\(\s*\'([^\']+?\.m3u8)\'\s*\)"', re.IGNORECASE)
        self.tag_pattern = re.compile(r'<tba[^>]*class="ergl"[^>]*>([^<]+\.m3u8)</tba>', re.IGNORECASE)

    def generate_random_hash(self):
        """ç”Ÿæˆéšæœºå“ˆå¸Œå€¼"""
        random_str = str(random.random())
        return hashlib.md5(random_str.encode()).hexdigest()[:8]

    def search_single_page(self, keyword, page, interval):
        """æœç´¢å•é¡µå†…å®¹å¹¶æ·»åŠ é—´éš”"""
        try:
            params = {
                'iptv': keyword,
                'l': self.generate_random_hash(),
                'page': page
            } if page > 1 else {
                'iptv': keyword,
                'l': self.generate_random_hash()
            }
            
            print(f"æ­£åœ¨å¤„ç†: {keyword} ç¬¬ {page} é¡µ")
            response = self.session.get(
                self.base_url, 
                params=params, 
                timeout=self.request_timeout
            )
            response.raise_for_status()
            
            print(f"æˆåŠŸè·å– {keyword} ç¬¬ {page} é¡µï¼ŒçŠ¶æ€ç : {response.status_code}")
            return self.parse_links_only(response.text, keyword)
            
        except requests.exceptions.RequestException as e:
            print(f"è¯·æ±‚å¤±è´¥: {keyword} ç¬¬ {page} é¡µ - {e}")
        except Exception as e:
            print(f"è§£æå¤±è´¥: {keyword} ç¬¬ {page} é¡µ - {e}")
        finally:
            # ç¡®ä¿å³ä½¿å‡ºé”™ä¹Ÿæ‰§è¡Œé—´éš”ç­‰å¾…
            if interval > 0 and page > 1:
                print(f"ç­‰å¾… {interval} ç§’åç»§ç»­...")
                time.sleep(interval)
        return []

    def parse_links_only(self, html_content, source):
        """ä¼˜åŒ–åçš„é“¾æ¥è§£æï¼ˆä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼‰"""
        found_links = []
        all_links = set()
        
        # åˆå¹¶æ‰€æœ‰æ‰¾åˆ°çš„é“¾æ¥
        for pattern in [self.m3u8_pattern, self.onclick_pattern, self.tag_pattern]:
            matches = pattern.findall(html_content)
            for link in matches:
                if not link.startswith(('http://', 'https://')):
                    link = 'https:' + link if link.startswith('//') else link
                all_links.add(link)
        
        # çº¿ç¨‹å®‰å…¨åœ°æ·»åŠ é“¾æ¥
        with self.lock:
            for link in all_links:
                found_links.append({
                    'url': link,
                    'source': source
                })
        
        return found_links

    def run(self, keywords=None, pages=4, interval=8):
        """ä¼˜åŒ–åçš„ä¸»è¿è¡Œé€»è¾‘ï¼ˆå¹¶å‘å¤„ç†ï¼‰"""
        if not keywords:
            keywords = ["æ¹–å—å«è§†", "æµ™æ±Ÿå«è§†", "æ±Ÿè‹å«è§†", "ä¸œæ–¹å«è§†", "åŒ—äº¬å«è§†"]
        
        self.all_links = []
        total_links = 0
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†ä¸åŒé¢‘é“
        with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:
            futures = []
            for keyword in keywords:
                # æ¯ä¸ªé¢‘é“ç‹¬ç«‹å¤„ç†æ‰€æœ‰é¡µé¢
                for page in range(1, pages + 1):
                    futures.append(executor.submit(
                        self.search_single_page,
                        keyword,
                        page,
                        interval if page > 1 else 0  # ç¬¬ä¸€é¡µä¸éœ€è¦ç­‰å¾…
                    ))
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future in concurrent.futures.as_completed(futures):
                links = future.result()
                if links:
                    with self.lock:
                        self.all_links.extend(links)
        
        # å»é‡å¤„ç†
        seen_urls = set()
        unique_links = []
        for item in self.all_links:
            if item['url'] not in seen_urls:
                unique_links.append(item)
                seen_urls.add(item['url'])
        
        # ä¿å­˜ç»“æœ
        output_file, total_count = self.save_to_m3u(unique_links)
        return output_file, unique_links, total_count

    def save_to_m3u(self, links_data, filename="wstv.m3u", output_dir="output"):
        """ä¿å­˜ç»“æœä¸ºM3Uæ ¼å¼æ–‡ä»¶"""
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        filepath = os.path.join(output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('#EXTM3U\n')
            for item in links_data:
                f.write(f'#EXTINF:-1 tvg-id="" tvg-name="{item["source"]}" tvg-logo="" group-title="å«è§†",{item["source"]}\n')
                f.write(f'{item["url"]}\n')
        
        print(f"æˆåŠŸä¿å­˜ {len(links_data)} ä¸ªé“¾æ¥åˆ° {filepath}")
        return filepath, len(links_data)

def main():
    """ä¸»å‡½æ•°"""
    print("Tonkiang.us IPTVçˆ¬è™«å¯åŠ¨ - å«è§†é¢‘é“ä¼˜åŒ–ç‰ˆ")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().isoformat()}")
    
    crawler = TonkiangCrawler()
    
    # é…ç½®å‚æ•° - å«è§†é¢‘é“
    search_keywords = [
        "å®‰å¾½å«è§†", "åŒ—äº¬å«è§†", "é‡åº†å«è§†", "ä¸œå—å«è§†", "å¹¿ä¸œå«è§†",
        "å¹¿è¥¿å«è§†", "è´µå·å«è§†", "æ²³åŒ—å«è§†", "æ²³å—å«è§†", "é»‘é¾™æ±Ÿå«è§†",
        "æ¹–åŒ—å«è§†", "æ¹–å—å«è§†", "å‰æ—å«è§†", "æ±Ÿè‹å«è§†", "æ±Ÿè¥¿å«è§†",
        "è¾½å®å«è§†", "å†…è’™å¤å«è§†", "å±±ä¸œå«è§†", "å±±è¥¿å«è§†", "é™•è¥¿å«è§†",
        "ä¸Šæµ·ä¸œæ–¹å«è§†", "å››å·å«è§†", "å¤©æ´¥å«è§†", "è¥¿è—å«è§†", "æ–°ç–†å«è§†",
        "äº‘å—å«è§†", "æµ™æ±Ÿå«è§†", "æ·±åœ³å«è§†"
    ]
    pages_to_crawl = 4
    request_interval = 8
    
    try:
        output_file, all_links, total_count = crawler.run(
            search_keywords, 
            pages_to_crawl, 
            request_interval
        )
        
        if output_file:
            print("\nâœ… çˆ¬å–å®Œæˆï¼")
            print(f"ğŸ“ M3Uæ–‡ä»¶: {output_file}")
            print(f"âœ… æ€»é“¾æ¥æ•°: {total_count} ä¸ª")
            
            # ç»Ÿè®¡å„é¢‘é“é“¾æ¥æ•°é‡
            tv_counts = {}
            for item in all_links:
                tv_counts[item['source']] = tv_counts.get(item['source'], 0) + 1
            
            print("\nå„é¢‘é“é“¾æ¥æ•°é‡ç»Ÿè®¡:")
            for tv, count in sorted(tv_counts.items(), key=lambda x: x[1], reverse=True):
                print(f"{tv}: {count} ä¸ªé“¾æ¥")
            
            # GitHub Actionsè¾“å‡º
            if os.getenv('GITHUB_ACTIONS') == 'true':
                with open(os.environ['GITHUB_OUTPUT'], 'a') as fh:
                    print(f'output_file={output_file}', file=fh)
                    print(f'total_links={total_count}', file=fh)
        else:
            print("\nâŒ çˆ¬å–å¤±è´¥ï¼Œæœªæ‰¾åˆ°ä»»ä½•é“¾æ¥")
            exit(1)
            
    except Exception as e:
        print(f"\nâŒ çˆ¬è™«æ‰§è¡Œå‡ºé”™: {e}")
        exit(1)

if __name__ == "__main__":
    main()
