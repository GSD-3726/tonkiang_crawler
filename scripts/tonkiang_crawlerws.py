#!/usr/bin/env python3
"""
Tonkiang.us IPTVçˆ¬è™« - æŒ‰å«è§†é¢‘é“å·æœç´¢å¹¶è¾“å‡ºå¯¹åº”é“¾æ¥
"""

import requests
import re
import os
import time
import random
import hashlib
from datetime import datetime
from urllib.parse import urlencode
import concurrent.futures

class TonkiangCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.base_url = "https://tonkiang.us/"
        self.request_timeout = (5, 15)
        self.all_links = []  # å­˜å‚¨æ‰€æœ‰æ‰¾åˆ°çš„é“¾æ¥

    def generate_random_hash(self):
        """ç”Ÿæˆéšæœºå“ˆå¸Œå€¼"""
        random_str = str(random.random())
        return hashlib.md5(random_str.encode()).hexdigest()[:8]

    def search_iptv_page(self, keyword="æ¹–å—å«è§†", page=1):
        """æœç´¢æŒ‡å®šé¡µé¢çš„IPTVé¢‘é“"""
        params = {
            'iptv': keyword,
            'l': self.generate_random_hash()
        }
        
        # æ·»åŠ åˆ†é¡µå‚æ•°
        if page > 1:
            params['page'] = page
        
        try:
            print(f"æ­£åœ¨æœç´¢: {keyword} ç¬¬ {page} é¡µ")
            
            response = self.session.get(
                self.base_url, 
                params=params, 
                timeout=self.request_timeout
            )
            response.raise_for_status()
            
            print(f"ç¬¬ {page} é¡µè·å–æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
            return self.parse_links_only(response.text, keyword)
            
        except requests.exceptions.Timeout:
            print(f"ç¬¬ {page} é¡µè¯·æ±‚è¶…æ—¶")
            return []
        except requests.exceptions.RequestException as e:
            print(f"ç¬¬ {page} é¡µè¯·æ±‚é”™è¯¯: {e}")
            return []
        except Exception as e:
            print(f"ç¬¬ {page} é¡µè§£æé”™è¯¯: {e}")
            return []

    def parse_links_only(self, html_content, source):
        """åªè§£æM3U8é“¾æ¥ï¼Œä¸å°è¯•åŒ¹é…é¢‘é“åç§°"""
        found_links = []
        
        # æŸ¥æ‰¾æ‰€æœ‰M3U8é“¾æ¥
        m3u8_pattern = r'https?://[^\s<>"]+?\.m3u8(?:\?[^\s<>"]*)?'
        m3u8_links = re.findall(m3u8_pattern, html_content, re.IGNORECASE)
        
        # æŸ¥æ‰¾onclickäº‹ä»¶ä¸­çš„M3U8é“¾æ¥
        onclick_pattern = r'onclick="glshle\(\s*\'([^\']+?\.m3u8)\'\s*\)"'
        onclick_links = re.findall(onclick_pattern, html_content, re.IGNORECASE)
        
        # æŸ¥æ‰¾ç‰¹å®šæ ‡ç­¾ä¸­çš„M3U8é“¾æ¥
        tag_pattern = r'<tba[^>]*class="ergl"[^>]*>([^<]+\.m3u8)</tba>'
        tag_links = re.findall(tag_pattern, html_content, re.IGNORECASE)
        
        # åˆå¹¶æ‰€æœ‰æ‰¾åˆ°çš„é“¾æ¥
        all_links = list(set(m3u8_links + onclick_links + tag_links))
        
        # å¤„ç†é“¾æ¥
        for link in all_links:
            # ç¡®ä¿é“¾æ¥æ˜¯å®Œæ•´çš„URL
            if not link.startswith(('http://', 'https://')):
                if link.startswith('//'):
                    link = 'https:' + link
                else:
                    continue
            
            found_links.append({
                'url': link,
                'source': source
            })
            print(f"æ‰¾åˆ°é“¾æ¥: {link}")
        
        return found_links

    def search_multiple_pages(self, keyword="æ¹–å—å«è§†", pages=2, interval=8):
        """æœç´¢å¤šé¡µå†…å®¹"""
        all_links = []
        
        for page in range(1, pages + 1):
            print(f"\n{'='*50}")
            print(f"å¼€å§‹å¤„ç†ç¬¬ {page} é¡µ")
            print(f"{'='*50}")
            
            links = self.search_iptv_page(keyword, page)
            
            if links:
                print(f"ç¬¬ {page} é¡µæ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
                all_links.extend(links)
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€é¡µï¼Œç­‰å¾…æŒ‡å®šçš„é—´éš”æ—¶é—´
                if page < pages:
                    print(f"ç­‰å¾… {interval} ç§’åç»§ç»­ä¸‹ä¸€é¡µ...")
                    time.sleep(interval)
            else:
                print(f"ç¬¬ {page} é¡µæœªæ‰¾åˆ°é“¾æ¥")
                break  # å¦‚æœæŸä¸€é¡µæ²¡æ‰¾åˆ°å†…å®¹ï¼Œåœæ­¢çˆ¬å–
        
        return all_links

    def save_to_m3u(self, links_data, filename="wstv.m3u", output_dir="output"):
        """ä¿å­˜ç»“æœä¸ºM3Uæ ¼å¼æ–‡ä»¶"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            print(f"åˆ›å»ºç›®å½•: {output_dir}")
        
        filepath = os.path.join(output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            # å†™å…¥M3Uæ–‡ä»¶å¤´
            f.write('#EXTM3U\n')
            
            # å†™å…¥æ¯ä¸ªé“¾æ¥
            for item in links_data:
                link = item['url']
                source = item['source']
                
                # ä½¿ç”¨æœç´¢å…³é”®è¯ä½œä¸ºé¢‘é“åç§°
                f.write(f'#EXTINF:-1 tvg-id="" tvg-name="{source}" tvg-logo="" group-title="å«è§†",{source}\n')
                f.write(f'{link}\n')
                print(f"å·²æ·»åŠ é“¾æ¥: {source} -> {link}")
        
        print(f"æˆåŠŸä¿å­˜ {len(links_data)} ä¸ªé“¾æ¥åˆ° {filepath}")
        return filepath, len(links_data)

    def run(self, keywords=None, pages=2, interval=8):
        """è¿è¡Œçˆ¬è™«"""
        if not keywords:
            keywords = ["æ¹–å—å«è§†", "æµ™æ±Ÿå«è§†", "æ±Ÿè‹å«è§†", "ä¸œæ–¹å«è§†", "åŒ—äº¬å«è§†"]
        
        # æ¸…ç©ºä¹‹å‰çš„é“¾æ¥åˆ—è¡¨
        self.all_links = []
        
        # æ”¶é›†æ‰€æœ‰é“¾æ¥
        for keyword in keywords:
            print(f"\n{'='*50}")
            print(f"å¼€å§‹å¤„ç†å…³é”®è¯: {keyword}")
            print(f"{'='*50}")
            
            links = self.search_multiple_pages(keyword, pages, interval)
            
            if links:
                print(f"ä¸ºå…³é”®è¯ '{keyword}' æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
                self.all_links.extend(links)
            else:
                print(f"å…³é”®è¯ '{keyword}' æœªæ‰¾åˆ°é“¾æ¥")
        
        if not self.all_links:
            print("æœªæ‰¾åˆ°ä»»ä½•é“¾æ¥")
            return None, [], 0
            
        # å»é‡å¤„ç†
        seen_urls = set()
        unique_links = []
        for item in self.all_links:
            if item['url'] not in seen_urls:
                unique_links.append(item)
                seen_urls.add(item['url'])
        
        print(f"\næ€»å…±æ‰¾åˆ° {len(unique_links)} ä¸ªå”¯ä¸€é“¾æ¥")
        
        # ä¿å­˜ç»“æœä¸ºM3Uæ ¼å¼
        output_file, total_count = self.save_to_m3u(
            unique_links, 
            "wstv.m3u", 
            "output"
        )
        
        return output_file, unique_links, total_count

def main():
    """ä¸»å‡½æ•°"""
    print("Tonkiang.us IPTVçˆ¬è™«å¯åŠ¨ - å«è§†é¢‘é“")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().isoformat()}")
    
    crawler = TonkiangCrawler()
    
    # é…ç½®å‚æ•° - å«è§†é¢‘é“
    search_keywords = [
    "å®‰å¾½å«è§†",   # å®‰å¾½å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "åŒ—äº¬å«è§†",   # åŒ—äº¬å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "é‡åº†å«è§†",   # é‡åº†å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "ä¸œå—å«è§†",   # ç¦å»ºå¹¿æ’­å½±è§†é›†å›¢ç»¼åˆé¢‘é“
    "å¹¿ä¸œå«è§†",   # å¹¿ä¸œå¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "å¹¿è¥¿å«è§†",   # å¹¿è¥¿å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "è´µå·å«è§†",   # è´µå·å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "æ²³åŒ—å«è§†",   # æ²³åŒ—å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "æ²³å—å«è§†",   # æ²³å—å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "é»‘é¾™æ±Ÿå«è§†", # é»‘é¾™æ±Ÿå¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "æ¹–åŒ—å«è§†",   # æ¹–åŒ—å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "æ¹–å—å«è§†",   # æ¹–å—å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "å‰æ—å«è§†",   # å‰æ—å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "æ±Ÿè‹å«è§†",   # æ±Ÿè‹çœå¹¿æ’­ç”µè§†æ€»å°ç»¼åˆé¢‘é“
    "æ±Ÿè¥¿å«è§†",   # æ±Ÿè¥¿å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "è¾½å®å«è§†",   # è¾½å®å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "å†…è’™å¤å«è§†", # å†…è’™å¤å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "å±±ä¸œå«è§†",   # å±±ä¸œå¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "å±±è¥¿å«è§†",   # å±±è¥¿å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "é™•è¥¿å«è§†",   # é™•è¥¿å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "ä¸Šæµ·ä¸œæ–¹å«è§†", # ä¸Šæµ·å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "å››å·å«è§†",   # å››å·å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "å¤©æ´¥å«è§†",   # å¤©æ´¥å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "è¥¿è—å«è§†",   # è¥¿è—å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "æ–°ç–†å«è§†",   # æ–°ç–†å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "äº‘å—å«è§†",   # äº‘å—å¹¿æ’­ç”µè§†å°ç»¼åˆé¢‘é“
    "æµ™æ±Ÿå«è§†",   # æµ™æ±Ÿå¹¿æ’­ç”µè§†é›†å›¢ç»¼åˆé¢‘é“
    "æ·±åœ³å«è§†"   # æ·±åœ³å¹¿æ’­ç”µå½±ç”µè§†é›†å›¢ç»¼åˆé¢‘é“
   
    ]
    pages_to_crawl = 6  # çˆ¬å–5é¡µ
    request_interval = 8  # 8ç§’é—´éš”
    
    try:
        output_file, all_links, total_count = crawler.run(
            search_keywords, 
            pages_to_crawl, 
            request_interval
        )
        
        if output_file:
            print(f"\nâœ… çˆ¬å–å®Œæˆï¼")
            print(f"ğŸ“ M3Uæ–‡ä»¶: {output_file}")
            print(f"âœ… æ€»é“¾æ¥æ•°: {total_count} ä¸ª")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            tv_counts = {}
            for item in all_links:
                source = item['source']
                tv_counts[source] = tv_counts.get(source, 0) + 1
            
            print("\nå„é¢‘é“é“¾æ¥æ•°é‡ç»Ÿè®¡:")
            for tv, count in sorted(tv_counts.items()):
                print(f"{tv}: {count} ä¸ªé“¾æ¥")
            
            # åœ¨GitHub Actionsç¯å¢ƒä¸­è®¾ç½®è¾“å‡ºå˜é‡
            if os.getenv('GITHUB_ACTIONS') == 'true':
                with open(os.environ['GITHUB_OUTPUT'], 'a') as fh:
                    print(f'output_file={output_file}', file=fh)
                    print(f'total_links={total_count}', file=fh)
        else:
            print("\nâŒ çˆ¬å–å¤±è´¥ï¼Œæœªæ‰¾åˆ°ä»»ä½•é“¾æ¥")
            exit(1)
            
    except Exception as e:
        print(f"\nâŒ çˆ¬è™«æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

if __name__ == "__main__":
    main()
