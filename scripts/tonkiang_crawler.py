#!/usr/bin/env python3
"""
Tonkiang.us IPTVçˆ¬è™« - ä¼˜åŒ–ç‰ˆï¼ˆGitHub Actionsä¸“ç”¨ï¼‰
"""

import requests
import re
import os
import random
import hashlib
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
import threading

class TonkiangCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.base_url = "https://tonkiang.us/"
        self.request_timeout = (5, 15)
        self.all_links = []
        self.lock = threading.Lock()

    @lru_cache(maxsize=100)
    def generate_random_hash(self):
        """å¸¦ç¼“å­˜çš„éšæœºå“ˆå¸Œç”Ÿæˆ"""
        return hashlib.md5(str(random.random()).encode()).hexdigest()[:8]

    def search_iptv_page(self, keyword, page):
        """å•é¡µæœç´¢ï¼ˆçº¿ç¨‹å®‰å…¨ç‰ˆï¼‰"""
        try:
            params = {
                'iptv': keyword,
                'l': self.generate_random_hash(),
                'page': page if page > 1 else None
            }
            
            response = self.session.get(
                self.base_url,
                params=params,
                timeout=self.request_timeout
            )
            response.raise_for_status()
            return self.parse_links_only(response.text, keyword)
            
        except Exception as e:
            print(f"âš ï¸ {keyword} ç¬¬{page}é¡µé”™è¯¯: {str(e)}")
            return []

    def parse_links_only(self, html_content, source):
        """å¸¦æ¥æºæ ‡æ³¨çš„é“¾æ¥è§£æ"""
        patterns = [
            r'https?://[^\s<>"]+?\.m3u8(?:\?[^\s<>"]*)?',
            r'onclick="glshle\(\s*\'([^\']+?\.m3u8)\'\s*\)"',
            r'<tba[^>]*class="ergl"[^>]*>([^<]+\.m3u8)</tba>'
        ]
        
        links = set()
        for pattern in patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for link in matches:
                if not link.startswith(('http://', 'https://')):
                    link = 'https:' + link if link.startswith('//') else None
                if link:
                    links.add((link, source))
        return list(links)

    def verify_m3u8_batch(self, links_batch):
        """æ‰¹é‡éªŒè¯é“¾æ¥æœ‰æ•ˆæ€§"""
        valid_links = []
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = {executor.submit(self._verify_single, link): (link, source) 
                      for link, source in links_batch}
            for future in as_completed(futures):
                link, source = futures[future]
                try:
                    if future.result():
                        valid_links.append({'url': link, 'source': source})
                except:
                    pass
        return valid_links

    def _verify_single(self, url):
        """å•é“¾æ¥éªŒè¯ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        try:
            with self.session.head(url, timeout=(3, 5), allow_redirects=True) as resp:
                return resp.status_code == 200 and 'mpegurl' in resp.headers.get('content-type', '')
        except:
            return False

    def run_concurrent(self, keywords, pages=2):
        """å¹¶å‘æ‰§è¡Œä¸»é€»è¾‘"""
        with ThreadPoolExecutor(max_workers=3) as executor:
            # ç¬¬ä¸€é˜¶æ®µï¼šå¹¶å‘çˆ¬å–
            futures = []
            for keyword in keywords:
                futures.append(executor.submit(
                    self._process_keyword,
                    keyword,
                    pages
                ))
            
            # ç¬¬äºŒé˜¶æ®µï¼šæ”¶é›†ç»“æœ
            for future in as_completed(futures):
                self.all_links.extend(future.result())
            
            # ç¬¬ä¸‰é˜¶æ®µï¼šæ‰¹é‡éªŒè¯
            if self.all_links:
                self.all_links = self.verify_m3u8_batch(self.all_links)

    def _process_keyword(self, keyword, pages):
        """å•ä¸ªå…³é”®è¯å¤„ç†æµç¨‹"""
        links = []
        with ThreadPoolExecutor(max_workers=2) as page_executor:
            page_futures = [page_executor.submit(
                self.search_iptv_page,
                keyword,
                page
            ) for page in range(1, pages+1)]
            
            for future in as_completed(page_futures):
                links.extend(future.result())
        return links

    def save_results(self, filename="ysws.m3u"):
        """ä¿å­˜ä¼˜åŒ–åçš„ç»“æœ"""
        os.makedirs("output", exist_ok=True)
        filepath = os.path.join("output", filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('#EXTM3U\n')
            for item in sorted(self.all_links, key=lambda x: x['source']):
                f.write(f'#EXTINF:-1 tvg-id="" tvg-name="{item["source"]}" tvg-logo="" group-title="CCTV",{item["source"]}\n')
                f.write(f'{item["url"]}\n')
        
        print(f"æˆåŠŸä¿å­˜ {len(self.all_links)} ä¸ªæœ‰æ•ˆé“¾æ¥åˆ° {filepath}")
        return filepath

def main():
    """ä¸»å‡½æ•°"""
    print("Tonkiang.us IPTVçˆ¬è™«å¯åŠ¨")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().isoformat()}")
    
    crawler = TonkiangCrawler()
    
    # é…ç½®å‚æ•°
    search_keywords = [
        "CCTV1", "CCTV2", "CCTV3", "CCTV4", "CCTV5",
        "CCTV6", "CCTV7", "CCTV8", "CCTV9", "CCTV10",
        "CCTV11", "CCTV12", "CCTV13", "CCTV14", "CCTV15",
        "CCTV16", "CCTV17"
    ]
    pages_to_crawl = 4
    request_interval = 8
    
    try:
        # å¹¶å‘æ‰§è¡Œçˆ¬å–
        crawler.run_concurrent(search_keywords, pages_to_crawl)
        
        # ä¿å­˜ç»“æœ
        output_file = crawler.save_results()
        
        print(f"\nâœ… çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“ M3Uæ–‡ä»¶: {output_file}")
        print(f"âœ… æœ‰æ•ˆé“¾æ¥: {len(crawler.all_links)} ä¸ª")
        
        # åœ¨GitHub Actionsç¯å¢ƒä¸­è®¾ç½®è¾“å‡ºå˜é‡
        if os.getenv('GITHUB_ACTIONS') == 'true':
            with open(os.environ['GITHUB_OUTPUT'], 'a') as fh:
                print(f'output_file={output_file}', file=fh)
                print(f'total_links={len(crawler.all_links)}', file=fh)
                print(f'valid_links={len(crawler.all_links)}', file=fh)
                
    except Exception as e:
        print(f"\nâŒ çˆ¬è™«æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

if __name__ == "__main__":
    main()

