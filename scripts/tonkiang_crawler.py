#!/usr/bin/env python3
"""
Tonkiang.us IPTVçˆ¬è™« - ä¼˜åŒ–ç‰ˆï¼ˆGitHub Actionsä¸“ç”¨ï¼‰
"""

import requests
import re
import os
import random
import hashlib
import time
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
import threading

class TonkiangCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.base_url = "https://tonkiang.us/"
        self.request_timeout = (5, 15)
        self.all_links = []
        self.lock = threading.Lock()
        self.print_lock = threading.Lock()

    def print_with_lock(self, message):
        with self.print_lock:
            print(message)

    @lru_cache(maxsize=100)
    def generate_random_hash(self):
        return hashlib.md5(str(random.random()).encode()).hexdigest()[:8]

    def search_iptv_page(self, keyword, page):
        try:
            wait_time = random.uniform(1, 3)
            self.print_with_lock(f"ç­‰å¾… {wait_time:.2f} ç§’åå¼€å§‹æœç´¢: {keyword} ç¬¬ {page} é¡µ")
            time.sleep(wait_time)
            
            params = {
                'iptv': keyword,
                'l': self.generate_random_hash(),
                'page': page if page > 1 else None
            }
            
            self.print_with_lock(f"æ­£åœ¨æœç´¢: {keyword} ç¬¬ {page} é¡µ")
            
            response = self.session.get(
                self.base_url,
                params=params,
                timeout=self.request_timeout
            )
            response.raise_for_status()
            
            self.print_with_lock(f"ç¬¬ {page} é¡µè·å–æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
            parsed_links = self.parse_links_only(response.text, keyword)
            
            # å°†å…ƒç»„åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            return [{'url': link, 'source': source} for link, source in parsed_links]
            
        except Exception as e:
            self.print_with_lock(f"âš ï¸ {keyword} ç¬¬{page}é¡µé”™è¯¯: {str(e)}")
            return []

    def parse_links_only(self, html_content, source):
        self.print_with_lock(f"å¼€å§‹è§£æ {source} çš„é¡µé¢å†…å®¹")
        
        patterns = [
            r'https?://[^\s<>"]+?\.m3u8(?:\?[^\s<>"]*)?',
            r'onclick="glshle\(\s*\'([^\']+?\.m3u8)\'\s*\)"',
            r'<tba[^>]*class="ergl"[^>]*>([^<]+\.m3u8)</tba>'
        ]
        
        links = set()
        for pattern in patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for link in matches:
                if not link.startswith(('http://', 'https://')):
                    link = 'https:' + link if link.startswith('//') else None
                if link:
                    links.add((link, source))
                    self.print_with_lock(f"æ‰¾åˆ°é“¾æ¥: {link}")
        
        self.print_with_lock(f"ä¸º {source} æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
        return list(links)

    def verify_m3u8_batch(self, links_batch):
        """æ‰¹é‡éªŒè¯å­—å…¸åˆ—è¡¨ä¸­çš„é“¾æ¥"""
        self.print_with_lock(f"å¼€å§‹æ‰¹é‡éªŒè¯ {len(links_batch)} ä¸ªé“¾æ¥")
        valid_links = []
        with ThreadPoolExecutor(max_workers=5) as executor:
            # ç›´æ¥ä½¿ç”¨å­—å…¸ä¸­çš„urlå­—æ®µ
            futures = {executor.submit(self._verify_single, item['url']): item for item in links_batch}
            for future in as_completed(futures):
                original_item = futures[future]
                try:
                    if future.result():
                        valid_links.append(original_item)
                        self.print_with_lock(f"âœ“ éªŒè¯é€šè¿‡: {original_item['url']}")
                    else:
                        self.print_with_lock(f"âœ— éªŒè¯å¤±è´¥: {original_item['url']}")
                except Exception as e:
                    self.print_with_lock(f"éªŒè¯é“¾æ¥æ—¶å‡ºé”™ {original_item['url']}: {e}")
        
        self.print_with_lock(f"æ‰¹é‡éªŒè¯å®Œæˆï¼Œæœ‰æ•ˆé“¾æ¥: {len(valid_links)} ä¸ª")
        return valid_links

    def _verify_single(self, url):
        try:
            wait_time = random.uniform(0.5, 1.5)
            time.sleep(wait_time)
            
            # ä½¿ç”¨GETè€Œä¸æ˜¯HEADï¼Œå› ä¸ºæœ‰äº›æœåŠ¡å™¨å¯èƒ½ä¸æ”¯æŒHEADæˆ–è¿”å›ä¸åŒçš„å†…å®¹ç±»å‹
            with self.session.get(url, timeout=(3, 5), stream=True) as resp:
                if resp.status_code != 200:
                    return False
                
                # æ£€æŸ¥å†…å®¹ç±»å‹æˆ–å†…å®¹æœ¬èº«
                content_type = resp.headers.get('content-type', '').lower()
                if 'mpegurl' in content_type or 'application/vnd.apple.mpegurl' in content_type:
                    return True
                
                # å¦‚æœä¸æ˜¯é¢„æœŸçš„å†…å®¹ç±»å‹ï¼Œæ£€æŸ¥å†…å®¹å‰å‡ ä¸ªå­—ç¬¦
                content_start = resp.raw.read(10).decode('utf-8', errors='ignore')
                return content_start.startswith('#EXTM3U')
        except:
            return False

    def run_concurrent(self, keywords, pages=2):
        self.print_with_lock(f"\n{'='*50}")
        self.print_with_lock("å¼€å§‹å¹¶å‘çˆ¬å–")
        self.print_with_lock(f"{'='*50}")
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = []
            for keyword in keywords:
                if len(futures) > 0:
                    delay = random.uniform(2, 5)
                    self.print_with_lock(f"ç­‰å¾… {delay:.2f} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªå…³é”®è¯")
                    time.sleep(delay)
                    
                futures.append(executor.submit(
                    self._process_keyword,
                    keyword,
                    pages
                ))
            
            for future in as_completed(futures):
                result = future.result()
                self.all_links.extend(result)
                self.print_with_lock(f"å®Œæˆä¸€ä¸ªå…³é”®è¯çš„å¤„ç†ï¼Œæ‰¾åˆ° {len(result)} ä¸ªé“¾æ¥")
            
            if self.all_links:
                self.print_with_lock(f"\nå¼€å§‹éªŒè¯æ‰€æœ‰æ‰¾åˆ°çš„ {len(self.all_links)} ä¸ªé“¾æ¥")
                # ç›´æ¥ä¼ é€’å­—å…¸åˆ—è¡¨è¿›è¡ŒéªŒè¯
                self.all_links = self.verify_m3u8_batch(self.all_links)
            else:
                self.print_with_lock("æœªæ‰¾åˆ°ä»»ä½•é“¾æ¥ï¼Œè·³è¿‡éªŒè¯é˜¶æ®µ")

    def _process_keyword(self, keyword, pages):
        self.print_with_lock(f"\nå¼€å§‹å¤„ç†å…³é”®è¯: {keyword}")
        links = []
        with ThreadPoolExecutor(max_workers=2) as page_executor:
            page_futures = [page_executor.submit(
                self.search_iptv_page,
                keyword,
                page
            ) for page in range(1, pages+1)]
            
            for future in as_completed(page_futures):
                links.extend(future.result())
        
        self.print_with_lock(f"å…³é”®è¯ {keyword} å¤„ç†å®Œæˆï¼Œå…±æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
        return links

    def save_results(self, filename="ysws.m3u"):
        self.print_with_lock(f"\nå¼€å§‹ä¿å­˜ç»“æœåˆ°æ–‡ä»¶: {filename}")
        os.makedirs("output", exist_ok=True)
        filepath = os.path.join("output", filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('#EXTM3U\n')
            for item in sorted(self.all_links, key=lambda x: x['source']):
                f.write(f'#EXTINF:-1 tvg-id="" tvg-name="{item["source"]}" tvg-logo="" group-title="CCTV",{item["source"]}\n')
                f.write(f'{item["url"]}\n')
        
        self.print_with_lock(f"æˆåŠŸä¿å­˜ {len(self.all_links)} ä¸ªæœ‰æ•ˆé“¾æ¥åˆ° {filepath}")
        return filepath

def main():
    print("Tonkiang.us IPTVçˆ¬è™«å¯åŠ¨")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().isoformat()}")
    
    crawler = TonkiangCrawler()
    
    search_keywords = [
        "CCTV1", "CCTV2", "CCTV3", "CCTV4", "CCTV5",
        "CCTV6", "CCTV7", "CCTV8", "CCTV9", "CCTV10",
        "CCTV11", "CCTV12", "CCTV13", "CCTV14", "CCTV15",
        "CCTV16", "CCTV17", "CETV1", "CETV2", "CETV3", 
        "CETV4"
    ]
    pages_to_crawl = 6
    
    try:
        crawler.run_concurrent(search_keywords, pages_to_crawl)
        output_file = crawler.save_results()
        
        print(f"\nâœ… çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“ M3Uæ–‡ä»¶: {output_file}")
        print(f"âœ… æœ‰æ•ˆé“¾æ¥: {len(crawler.all_links)} ä¸ª")
        
        tv_counts = {}
        for item in crawler.all_links:
            source = item['source']
            tv_counts[source] = tv_counts.get(source, 0) + 1
        
        print("\nå„é¢‘é“é“¾æ¥æ•°é‡ç»Ÿè®¡:")
        for tv, count in sorted(tv_counts.items()):
            print(f"{tv}: {count} ä¸ªé“¾æ¥")
        
        if os.getenv('GITHUB_ACTIONS') == 'true':
            with open(os.environ['GITHUB_OUTPUT'], 'a') as fh:
                print(f'output_file={output_file}', file=fh)
                print(f'total_links={len(crawler.all_links)}', file=fh)
                print(f'valid_links={len(crawler.all_links)}', file=fh)
                
    except Exception as e:
        print(f"\nâŒ çˆ¬è™«æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

if __name__ == "__main__":
    main()



